{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0a8deb-a4ce-46c8-a869-028b4e6f7407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aefec9e4-249b-43f6-a882-ba772536b707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "313560d1-7c96-44a8-97ac-372dfecdb19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # æ˜¾ç¤ºæ‰€æœ‰åˆ—\n",
    "pd.set_option('display.width', 1000)        # è®¾ç½®æ˜¾ç¤ºå®½åº¦\n",
    "pd.set_option('display.max_colwidth', 50)   # è®¾ç½®æ¯åˆ—æœ€å¤§å®½åº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6f5f313-0651-4504-94f5-96b2f196dcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… åˆå¹¶å®Œæˆï¼Œå…± 4232 è¡Œï¼Œä¿å­˜åˆ°ï¼š./us_all_original_videos.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# === 1. è®¾ç½®æ–‡ä»¶å¤¹è·¯å¾„ ===\n",
    "data_dir = \"./data/source_datas\"   # ä¿®æ”¹ä¸ºä½ è‡ªå·±çš„è·¯å¾„ï¼Œä¾‹å¦‚ \"./data/original/\"\n",
    "output_path = \"./us_all_original_videos.csv\"\n",
    "\n",
    "# === 2. è¦åˆå¹¶çš„æ–‡ä»¶åˆ—è¡¨ï¼ˆæ ¹æ®ä½ æä¾›çš„æˆªå›¾ï¼‰ ===\n",
    "file_list = [\n",
    "    \"us_comedy.csv\",\n",
    "    \"us_education.csv\",\n",
    "    \"us_entertainment.csv\",\n",
    "    \"us_gaming.csv\",\n",
    "    \"us_howto_style.csv\",\n",
    "    \"us_music.csv\",\n",
    "    \"us_news_politics.csv\",\n",
    "    \"us_science_technology.csv\",\n",
    "    \"us_sports.csv\",\n",
    "    \"us_travel_vlog.csv\"\n",
    "]\n",
    "\n",
    "# === 3. åˆå¹¶é€»è¾‘ ===\n",
    "dfs = []\n",
    "for file in file_list:\n",
    "    path = os.path.join(data_dir, file)\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # è‡ªåŠ¨è¯†åˆ«ç±»åˆ«åï¼ˆå»æ‰å‰ç¼€ \"us_\" å’Œæ‰©å±•åï¼‰\n",
    "    category = (\n",
    "        os.path.splitext(file)[0]\n",
    "        .replace(\"us_\", \"\")\n",
    "        .replace(\" 10.59.31PM\", \"\")  # é’ˆå¯¹é‚£ä¸ª music æ–‡ä»¶ç‰¹ä¾‹\n",
    "    )\n",
    "    df[\"category\"] = category\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "# åˆå¹¶æˆä¸€å¼ è¡¨\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# === 4. æ¸…ç† / æ’åº ===\n",
    "# å»é‡ video_id + upload_date\n",
    "if set([\"video_id\", \"upload_date\"]).issubset(merged_df.columns):\n",
    "    merged_df = merged_df.drop_duplicates(subset=[\"video_id\", \"upload_date\"])\n",
    "\n",
    "# æŒ‰ç±»åˆ«å’Œä¸Šä¼ æ—¥æœŸæ’åº\n",
    "if \"upload_date\" in merged_df.columns:\n",
    "    merged_df = merged_df.sort_values(by=[\"category\", \"upload_date\"])\n",
    "\n",
    "# === 5. å¯¼å‡º ===\n",
    "merged_df.to_csv(output_path, index=False)\n",
    "print(f\"âœ… åˆå¹¶å®Œæˆï¼Œå…± {merged_df.shape[0]} è¡Œï¼Œä¿å­˜åˆ°ï¼š{output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900170e1-e846-427a-a2b0-f5337f7b9c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "307bc2e2-4770-4ca5-bb16-d4a7676b30f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ æ‰¾åˆ° 55 ä¸ª follow-up æ–‡ä»¶\n",
      "âœ… followup_2025-10-12.csv: 3901 è¡Œ\n",
      "âœ… followup_2025-10-13.csv: 3890 è¡Œ\n",
      "âœ… followup_2025-10-14.csv: 3876 è¡Œ\n",
      "âœ… followup_2025-10-15.csv: 3861 è¡Œ\n",
      "âœ… followup_2025-10-16.csv: 3852 è¡Œ\n",
      "âœ… followup_2025-10-17.csv: 3848 è¡Œ\n",
      "âœ… followup_2025-10-18.csv: 3835 è¡Œ\n",
      "âœ… followup_2025-10-19.csv: 3829 è¡Œ\n",
      "âœ… followup_2025-10-20.csv: 3823 è¡Œ\n",
      "âœ… followup_2025-10-21.csv: 3811 è¡Œ\n",
      "âœ… followup_2025-10-22.csv: 3799 è¡Œ\n",
      "âœ… followup_2025-10-23.csv: 3788 è¡Œ\n",
      "âœ… followup_2025-10-24.csv: 3786 è¡Œ\n",
      "âœ… followup_2025-10-25.csv: 3777 è¡Œ\n",
      "âœ… followup_2025-10-26.csv: 3745 è¡Œ\n",
      "âœ… followup_2025-10-27.csv: 3759 è¡Œ\n",
      "âœ… followup_2025-10-28.csv: 3755 è¡Œ\n",
      "âœ… followup_2025-10-29.csv: 3753 è¡Œ\n",
      "âœ… followup_2025-10-30.csv: 3753 è¡Œ\n",
      "âœ… followup_2025-10-31.csv: 3747 è¡Œ\n",
      "âœ… followup_2025-11-01.csv: 3745 è¡Œ\n",
      "âœ… followup_2025-11-02.csv: 3744 è¡Œ\n",
      "âœ… followup_2025-11-03.csv: 3740 è¡Œ\n",
      "âœ… followup_2025-11-04.csv: 3735 è¡Œ\n",
      "âœ… followup_2025-11-05.csv: 3732 è¡Œ\n",
      "âœ… followup_2025-11-06.csv: 3729 è¡Œ\n",
      "âœ… followup_2025-11-07.csv: 3726 è¡Œ\n",
      "âœ… followup_2025-11-08.csv: 3724 è¡Œ\n",
      "âœ… followup_2025-11-09.csv: 3724 è¡Œ\n",
      "âœ… followup_2025-11-10.csv: 3720 è¡Œ\n",
      "âœ… followup_2025-11-11.csv: 3719 è¡Œ\n",
      "âœ… followup_2025-11-12.csv: 3716 è¡Œ\n",
      "âœ… followup_2025-11-13.csv: 3711 è¡Œ\n",
      "âœ… followup_2025-11-14.csv: 3704 è¡Œ\n",
      "âœ… followup_2025-11-15.csv: 3702 è¡Œ\n",
      "âœ… followup_2025-11-16.csv: 3701 è¡Œ\n",
      "âœ… followup_2025-11-17.csv: 3695 è¡Œ\n",
      "âœ… followup_2025-11-18.csv: 3690 è¡Œ\n",
      "âœ… followup_2025-11-19.csv: 3686 è¡Œ\n",
      "âœ… followup_2025-11-20.csv: 3676 è¡Œ\n",
      "âœ… followup_2025-11-21.csv: 3672 è¡Œ\n",
      "âœ… followup_2025-11-22.csv: 3668 è¡Œ\n",
      "âœ… followup_2025-11-23.csv: 3668 è¡Œ\n",
      "âœ… followup_2025-11-24.csv: 3666 è¡Œ\n",
      "âœ… followup_2025-11-25.csv: 3659 è¡Œ\n",
      "âœ… followup_2025-11-26.csv: 3654 è¡Œ\n",
      "âœ… followup_2025-11-27.csv: 3645 è¡Œ\n",
      "âœ… followup_2025-11-28.csv: 3636 è¡Œ\n",
      "âœ… followup_2025-11-29.csv: 3634 è¡Œ\n",
      "âœ… followup_2025-11-30.csv: 3633 è¡Œ\n",
      "âœ… followup_2025-12-01.csv: 3628 è¡Œ\n",
      "âœ… followup_2025-12-02.csv: 3622 è¡Œ\n",
      "âœ… followup_2025-12-03.csv: 3621 è¡Œ\n",
      "âœ… followup_2025-12-04.csv: 3619 è¡Œ\n",
      "âœ… followup_2025-12-05.csv: 3619 è¡Œ\n",
      "ğŸ“ˆ åˆå¹¶å®Œæˆï¼Œå…± 205151 è¡Œï¼Œ3901 ä¸ªè§†é¢‘\n",
      "ğŸ’¾ ä¿å­˜åˆ°: ./data/followup_all.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# === 1. è·¯å¾„è®¾ç½® ===\n",
    "followup_dir = \"./data/Followups\"         # ä½ çš„ follow-up æ–‡ä»¶å¤¹è·¯å¾„\n",
    "output_path  = \"./data/followup_all.csv\"  # è¾“å‡ºæ–‡ä»¶è·¯å¾„\n",
    "\n",
    "# === 2. è·å–æ‰€æœ‰ follow-up æ–‡ä»¶ ===\n",
    "followup_files = sorted(glob.glob(os.path.join(followup_dir, \"followup_*.csv\")))\n",
    "print(f\"ğŸ“‚ æ‰¾åˆ° {len(followup_files)} ä¸ª follow-up æ–‡ä»¶\")\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for f in followup_files:\n",
    "    try:\n",
    "        # ä»æ–‡ä»¶åæå–æ—¥æœŸ\n",
    "        date_str = os.path.basename(f).split(\"followup_\")[-1].split(\".csv\")[0]\n",
    "        date_str = date_str.replace(\"_\", \"-\")\n",
    "\n",
    "        # è¯»å– CSV\n",
    "        df = pd.read_csv(f)\n",
    "\n",
    "        # æ ‡å‡†åŒ–å¸¸è§åˆ—å\n",
    "        df.rename(columns={\n",
    "            \"view_count\": \"views\",\n",
    "            \"like_count\": \"likes\",\n",
    "            \"comment_count\": \"comments\"\n",
    "        }, inplace=True)\n",
    "\n",
    "        # æ·»åŠ æ—¥æœŸåˆ—\n",
    "        df[\"crawl_date\"] = pd.to_datetime(date_str)\n",
    "\n",
    "        dfs.append(df)\n",
    "        print(f\"âœ… {os.path.basename(f)}: {df.shape[0]} è¡Œ\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ è¯»å– {f} å‡ºé”™: {e}\")\n",
    "\n",
    "# === 3. åˆå¹¶ ===\n",
    "followup_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# === 4. å»é‡ã€æ’åº ===\n",
    "if {\"video_id\", \"crawl_date\"}.issubset(followup_all.columns):\n",
    "    followup_all.drop_duplicates(subset=[\"video_id\", \"crawl_date\"], inplace=True)\n",
    "    followup_all.sort_values(by=[\"video_id\", \"crawl_date\"], inplace=True)\n",
    "\n",
    "followup_all.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"ğŸ“ˆ åˆå¹¶å®Œæˆï¼Œå…± {followup_all.shape[0]} è¡Œï¼Œ{followup_all['video_id'].nunique()} ä¸ªè§†é¢‘\")\n",
    "\n",
    "# === 5. å¯¼å‡º ===\n",
    "followup_all.to_csv(output_path, index=False)\n",
    "print(f\"ğŸ’¾ ä¿å­˜åˆ°: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "223eded3-7f0c-4946-95b7-afc22e6a974d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åˆå¹¶å®Œæˆï¼å·²ä¿å­˜ master_with_followups.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === 1. è¯»å–ä¸» CSV ===\n",
    "master_df = pd.read_csv(\"./data/us_all_original_videos.csv\")\n",
    "\n",
    "# === 2. followup_all å·²ç»åœ¨ä½ çš„ç¯å¢ƒé‡Œ ===\n",
    "# ç¡®ä¿ video_id æ˜¯ strï¼ˆé¿å…ç±»å‹ä¸ä¸€è‡´å¯¼è‡´ merge å¤±è´¥ï¼‰\n",
    "master_df[\"video_id\"] = master_df[\"video_id\"].astype(str)\n",
    "followup_all[\"video_id\"] = followup_all[\"video_id\"].astype(str)\n",
    "\n",
    "# === 3. åˆå¹¶ï¼šä¸€æ¡è§†é¢‘æœ‰å¤šæ¡ followupï¼ˆä¸åŒ crawl_dateï¼‰ ===\n",
    "merged = master_df.merge(\n",
    "    followup_all,\n",
    "    on=\"video_id\",\n",
    "    how=\"left\"               # ä¸ä¸¢å¤±ä¸» CSV çš„ä»»ä½•è¡Œ\n",
    ")\n",
    "\n",
    "# === 4. æ’åºï¼ˆæŒ‰ video_id å’Œæ—¥æœŸï¼‰ ===\n",
    "if \"crawl_date\" in merged.columns:\n",
    "    merged.sort_values([\"video_id\", \"crawl_date\"], inplace=True)\n",
    "\n",
    "merged.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# === 5. ä¿å­˜ ===\n",
    "merged.to_csv(\"./master_with_followups.csv\", index=False)\n",
    "print(\"åˆå¹¶å®Œæˆï¼å·²ä¿å­˜ master_with_followups.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f22099-7d8f-4dcb-8bdd-a1765446c42d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485140e0-674f-44e6-8f03-f93024eb0235",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bdf411-19d5-437f-8dcf-36c56b789cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e819f9c-0250-4402-91dc-56667dd05488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------find the best popularity calculation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa85f559-1e3d-4128-814a-c6faa4ca5130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>views</th>\n",
       "      <th>likes</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>215129</th>\n",
       "      <td>--b_A-cZKK0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215130</th>\n",
       "      <td>--b_A-cZKK0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215131</th>\n",
       "      <td>--b_A-cZKK0</td>\n",
       "      <td>402.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215132</th>\n",
       "      <td>--b_A-cZKK0</td>\n",
       "      <td>407.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215133</th>\n",
       "      <td>--b_A-cZKK0</td>\n",
       "      <td>407.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           video_id  views  likes  comments\n",
       "215129  --b_A-cZKK0  290.0    5.0       0.0\n",
       "215130  --b_A-cZKK0  377.0    6.0       0.0\n",
       "215131  --b_A-cZKK0  402.0    6.0       0.0\n",
       "215132  --b_A-cZKK0  407.0    6.0       0.0\n",
       "215133  --b_A-cZKK0  407.0    6.0       0.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"./data/master_with_followups.csv\")\n",
    "\n",
    "# ç¡®ä¿æ—¥æœŸæ ¼å¼æ­£ç¡®\n",
    "df[\"crawl_date\"] = pd.to_datetime(df[\"crawl_date\"])\n",
    "\n",
    "# æŒ‰ video_id + date æ’åº\n",
    "df = df.sort_values([\"video_id\", \"crawl_date\"])\n",
    "\n",
    "# ä¸ºæ¯ä¸ª video_id æ‰¾ä¸‹ä¸€æ¬¡çˆ¬å–è§†å›¾æ•°\n",
    "df[\"views_next\"] = df.groupby(\"video_id\")[\"views\"].shift(-1)\n",
    "\n",
    "# ä¿ç•™å¿…è¦çš„åˆ—ï¼ˆåŒ…æ‹¬ views_next å’Œ crawl_dateï¼‰\n",
    "df = df[[\"video_id\", \"views\", \"likes\", \"comments\", \"views_next\", \"crawl_date\"]].dropna(subset=[\"video_id\", \"views\", \"likes\", \"comments\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9fdb3006-a25b-4f71-8bce-a43b896b3b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"log_views\"] = np.log1p(df[\"views\"])\n",
    "df[\"log_likes\"] = np.log1p(df[\"likes\"])\n",
    "df[\"log_comments\"] = np.log1p(df[\"comments\"])\n",
    "\n",
    "df[\"like_rate\"] = df[\"likes\"] / df[\"views\"].clip(lower=1)\n",
    "df[\"comment_rate\"] = df[\"comments\"] / df[\"views\"].clip(lower=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b65e15-8fed-4bd2-9d0e-683c656aad53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([256826.7580925 ,  39115.76474725, 162534.29624159])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# ä½¿ç”¨ log ç‰¹å¾é¢„æµ‹ views_nextï¼ˆæœªæ¥çš„ viewsï¼‰\n",
    "train_df = df.dropna(subset=[\"views_next\"])  # å»é™¤æ— æ ‡ç­¾æ ·æœ¬\n",
    "X = train_df[[\"log_views\",\"log_likes\",\"log_comments\"]]\n",
    "y = train_df[\"views_next\"]  # ä½¿ç”¨ views_next ä½œä¸ºç›®æ ‡å˜é‡\n",
    "\n",
    "reg = LinearRegression().fit(X, y)\n",
    "\n",
    "weights = reg.coef_\n",
    "intercept = reg.intercept_\n",
    "print(f\"å›å½’ç³»æ•°: {weights}\")\n",
    "print(f\"æˆªè·: {intercept}\")\n",
    "print(f\"è®­ç»ƒæ ·æœ¬æ•°: {len(train_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2a326a-fc8b-4133-a94a-96e900ac9800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# V1: åŸå§‹åŠ æƒå…¬å¼\n",
    "df[\"pop_V1\"] = df[\"views\"] + 5*df[\"likes\"] + 10*df[\"comments\"]\n",
    "\n",
    "# V2: ä½¿ç”¨å›å½’æ¨¡å‹é¢„æµ‹ views_nextï¼ˆåŒ…å«æˆªè·ï¼‰\n",
    "a, b, c = weights\n",
    "df[\"pop_V2\"] = a*df[\"log_views\"] + b*df[\"log_likes\"] + c*df[\"log_comments\"] + intercept\n",
    "\n",
    "# V3: åŸºäºæ¯”ç‡çš„æŒ‡æ ‡\n",
    "df[\"pop_V3\"] = df[\"like_rate\"] + 3 * df[\"comment_rate\"]\n",
    "\n",
    "# V4: æ‰‹åŠ¨åŠ æƒçš„ log ç‰¹å¾ï¼ˆä¿®å¤ï¼šåˆ é™¤é‡å¤å®šä¹‰ï¼‰\n",
    "df[\"pop_V4\"] = df[\"log_views\"] + 2*df[\"log_likes\"] + 3*df[\"log_comments\"]\n",
    "\n",
    "# V5: PCA é™ç»´\n",
    "pca = PCA(n_components=1)\n",
    "df[\"pop_V5\"] = pca.fit_transform(df[[\"log_views\",\"log_likes\",\"log_comments\"]]).flatten()\n",
    "\n",
    "# V6: ç»“åˆå¢é•¿ç‡çš„æŒ‡æ ‡ï¼ˆä¿®å¤ï¼šç¡®ä¿ views_next å­˜åœ¨ï¼‰\n",
    "if \"views_next\" in df.columns:\n",
    "    df[\"growth\"] = (df[\"views_next\"] - df[\"views\"]) / (df[\"views\"]+1)\n",
    "else:\n",
    "    df[\"growth\"] = 0\n",
    "df[\"pop_V6\"] = df[\"pop_V2\"] + 2 * df[\"growth\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d4bebc9e-c6b0-4413-b58c-54bc761e357e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['video_id', 'views', 'likes', 'comments', 'log_views', 'log_likes', 'log_comments', 'like_rate', 'comment_rate', 'pop_V1', 'pop_V2', 'pop_V3', 'pop_V4', 'pop_V5', 'growth', 'pop_V6'], dtype='object')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4ad77882-73c3-4428-906a-95947b93f8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(\"./data/master_with_followups.csv\")\n",
    "df1[\"crawl_date\"] = pd.to_datetime(df1[\"crawl_date\"])\n",
    "\n",
    "# æŒ‰è§†é¢‘æ’åº\n",
    "df1 = df1.sort_values([\"video_id\", \"crawl_date\"])\n",
    "\n",
    "# æ¯ä¸ªè§†é¢‘ç¬¬ä¸€å¤©çš„æ—¥æœŸ = day0\n",
    "df1[\"day0\"] = df1.groupby(\"video_id\")[\"crawl_date\"].transform(\"min\")\n",
    "\n",
    "# è®¡ç®— relative day\n",
    "df1[\"relative_day\"] = (df1[\"crawl_date\"] - df1[\"day0\"]).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "38990dca-ee78-4461-bc6b-e7d5695b2f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['video_id', 'id', 'title', 'description', 'hashtags', 'channel', 'published_at', 'category_id', 'duration', 'definition', 'category', 'views', 'likes', 'comments', 'crawl_date', 'day0', 'relative_day', 'views_1d', 'views_7d', 'views_30d', 'views_final']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: ç¡®ä¿æ—¥æœŸæ­£ç¡®ï¼ˆä½¿ç”¨ df1ï¼Œä¸æ˜¯ dfï¼‰\n",
    "df1[\"crawl_date\"] = pd.to_datetime(df1[\"crawl_date\"])\n",
    "\n",
    "# Step 2: pivot å¾—åˆ° daily views table\n",
    "pivot = df1.pivot_table(\n",
    "    index=\"video_id\",\n",
    "    columns=\"relative_day\",\n",
    "    values=\"views\",\n",
    "    aggfunc=\"max\"\n",
    ")\n",
    "\n",
    "# Step 3: æŠŠ columns å˜æˆ intï¼ˆä½ çš„ relative_day æœ¬æ¥æ˜¯ floatï¼‰\n",
    "pivot.columns = pivot.columns.astype(int)\n",
    "\n",
    "# Step 4: rename æœªæ¥è§†å›¾\n",
    "pivot = pivot.rename(columns={\n",
    "    1: \"views_1d\",\n",
    "    7: \"views_7d\",\n",
    "    30: \"views_30d\"\n",
    "})\n",
    "\n",
    "# Step 5: æœ€ç»ˆ viewsï¼ˆæœ€åä¸€å¤©ï¼‰\n",
    "pivot[\"views_final\"] = pivot.max(axis=1)\n",
    "\n",
    "# åªä¿ç•™å…³æ³¨çš„å¤©æ•°åˆ—\n",
    "keep_cols = [\"views_1d\", \"views_7d\", \"views_30d\", \"views_final\"]\n",
    "existing_cols = [c for c in keep_cols if c in pivot.columns]\n",
    "pivot = pivot[existing_cols].reset_index()  # å¸¦ä¸Š video_id ç”¨äº merge\n",
    "\n",
    "# Step 6: ä¿ç•™åŸ df1 ä¸­æ¯ä¸ªè§†é¢‘çš„é™æ€ä¿¡æ¯ï¼ˆå¦‚æ ‡é¢˜ã€æ ‡ç­¾ç­‰ï¼‰\n",
    "static = df1.sort_values(\"crawl_date\").groupby(\"video_id\").first().reset_index()\n",
    "\n",
    "# Step 7: merge\n",
    "full_df = static.merge(pivot, on=\"video_id\", how=\"left\")\n",
    "\n",
    "# Step 8: æŸ¥çœ‹ columns\n",
    "print(full_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fca2b627-7f6d-40da-9323-fadbb7083919",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = df.merge(pivot, on=\"video_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d877dd2d-43b5-4334-9686-975181a13238",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['video_id', 'views', 'likes', 'comments', 'views_next', 'crawl_date', 'log_views', 'log_likes', 'log_comments', 'like_rate', 'comment_rate', 'pop_V1', 'pop_V2', 'pop_V3', 'pop_V4', 'pop_V5', 'growth', 'pop_V6', 'views_1d', 'views_7d', 'views_30d', 'views_final']\n"
     ]
    }
   ],
   "source": [
    "print(full_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ae36e363-4188-4054-aacc-d012fad47aa1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 29\u001b[0m\n\u001b[1;32m     24\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m     25\u001b[0m     X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     28\u001b[0m model \u001b[38;5;241m=\u001b[39m LinearRegression()\n\u001b[0;32m---> 29\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     32\u001b[0m r2 \u001b[38;5;241m=\u001b[39m r2_score(y_test, pred)\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/sklearn/base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1363\u001b[0m     )\n\u001b[1;32m   1364\u001b[0m ):\n\u001b[0;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/sklearn/linear_model/_base.py:618\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    614\u001b[0m n_jobs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs\n\u001b[1;32m    616\u001b[0m accept_sparse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositive \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 618\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    628\u001b[0m has_sw \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_sw:\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/sklearn/utils/validation.py:2971\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2969\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m   2970\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2971\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2972\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m   2974\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/sklearn/utils/validation.py:1368\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1362\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1363\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1364\u001b[0m     )\n\u001b[1;32m   1366\u001b[0m ensure_all_finite \u001b[38;5;241m=\u001b[39m _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[0;32m-> 1368\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1385\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1387\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/sklearn/utils/validation.py:1105\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1100\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m while dim <= 2 is required\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1102\u001b[0m     )\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[0;32m-> 1105\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1114\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/sklearn/utils/validation.py:120\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/sklearn/utils/validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "\n",
    "# ä½ å…­ä¸ª popularity metrics\n",
    "pop_cols = [\"pop_V1\", \"pop_V2\", \"pop_V3\", \"pop_V4\", \"pop_V5\", \"pop_V6\"]\n",
    "\n",
    "# ä½ å››ä¸ªæœªæ¥çƒ­åº¦æŒ‡æ ‡\n",
    "targets = [\"views_1d\", \"views_7d\", \"views_30d\", \"views_final\"]\n",
    "\n",
    "# å­˜ç»“æœ\n",
    "results = {t: {} for t in targets}\n",
    "\n",
    "# å»æ‰æœªæ¥ view ä¸º NaN çš„è¡Œ\n",
    "clean_df = full_df.dropna(subset=targets)\n",
    "\n",
    "for t in targets:\n",
    "    for col in pop_cols:\n",
    "\n",
    "        X = clean_df[[col]]\n",
    "        y = clean_df[t]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.1, random_state=42\n",
    "        )\n",
    "\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        pred = model.predict(X_test)\n",
    "        r2 = r2_score(y_test, pred)\n",
    "\n",
    "        results[t][col] = r2\n",
    "\n",
    "# è¾“å‡ºç»“æœ\n",
    "print(\"\\n=== RÂ² scores for each popularity metric predicting future views ===\\n\")\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results)\n",
    "\n",
    "print(\"\\nBest metric per target:\")\n",
    "print(df_results.idxmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a58648-ffa5-4c6a-afdc-992ca3e085bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
